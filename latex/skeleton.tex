% to choose your degree
% please un-comment just one of the following
\documentclass[bsc,frontabs,twoside,singlespacing,parskip,deptreport]{infthesis}     % for BSc, BEng etc.
% \documentclass[minf,frontabs,twoside,singlespacing,parskip,deptreport]{infthesis}  % for MInf
\usepackage{cite}
\usepackage{graphicx}
\usepackage{float}

\begin{document}

\title{Event Ordering in News Articles}

\author{James Robert Friel}
% to choose your course
% please un-comment just one of the following
\course{Artificial Intelligence and Computer Science}
%\course{Artificial Intelligence and Software Engineering}
%\course{Artificial Intelligence and Mathematics}
%\course{Artificial Intelligence and Psychology }   
%\course{Artificial Intelligence with Psychology }   
%\course{Linguistics and Artificial Intelligence}    
%\course{Computer Science}
%\course{Software Engineering}
%\course{Computer Science and Electronics}    
%\course{Electronics and Software Engineering}    
%\course{Computer Science and Management Science}    
%\course{Computer Science and Mathematics}
%\course{Computer Science and Physics}  
%\course{Computer Science and Statistics}    

% to choose your report type
% please un-comment just one of the following
\project{Undergraduate Dissertation} % CS&E, E&SE, AI&L
%\project{Undergraduate Thesis} % AI%Psy
%\project{4th Year Disertation}

\date{\today}

\abstract{}
%TODO


\maketitle

\section*{Acknowledgements}
I'd Like to thank Greggs Bakery for being there for me through thick and thin.

\tableofcontents

%\pagenumbering{arabic}


\chapter{Introduction}
\section{The Problem}
Nominal data is not descriptive in nature. This lack of information makes it difficult to assign
a canonical ordering to.
The problem tackled within this dissertation is to test if the use of external data sources can
aid in the ordering of news article headlines.
We will be looking at the usefulness of Wikipedia as an external source in order to timeline
historical events.

Our event data comes from the ``Today in History'' dataset. Each of these data points consists
of an event and a date\\
$[$ ``Alaska becomes 49th State'', ``1959-01-01'' $]$


The problem tackled in this paper is based on the paper \cite{abend2015lexical} and several
techniques used are based from this paper. 

%Nominal data is descriptive in nature, making is difficult to assign an cannonical ordering to.
%The problem tackled in this disertation is the ordering of news article headlines to generate
%a most-probable traversal of a weighted directed graph of these events.
%This problem is based off of the paper \cite{abend2015lexical} and some
%techniques discussed and used henseforth are based off of this paper.


%Our data comes from the Wikipedia ``Today in History'' dataset and the nominal data is retrived
%from wikipedia articles.
%The problem explored within this thesis is that of graphing nature.
%Building a most probable path through each node in an edge-weighted directed graph
%, each node beinga single sentence describing an event from history.
%Examples of these events include:
%\say{Julian claendar begins at Greenwich mean noon}
%and \say{AT\&T broken up into 8 countries}

\section{Aims \& Objectives}

The aim of this project is to experiment with machine reading on external data sources to
discover if it is feasable to extract context from these sources to aid in the linear ordering
of historical events.

We aim to do this be experimenting with machine reading techniques to try and extract the most
relevent contextual information from our data sources.

With this information we intend to experiment with classification methods to optimise estimations
of local orderings before generating a graph from the returned data.

Using the graph generated from our classifier, we will attempt to find the optimim maximum spanning path
through every node. This path will be our estimated ordering of history.

We can use some evaluation technique to see how accurate our results are, compair those to not using
external data sources and to a human rate.


%The core aims of the project were: 
%\begin{itemize}
%  \item Investivage the usefulness of wikipedia text in feature generation
%  \item Generate probable dates For each event
%  \item Order these events linearly
%\end{itemize}

%The further goals of the project were to:
%\begin{itemize}
%  \item Generate edge-weighted directional graphs of the data
%  \item Construct probabilities of maximum spanning walks through the graph
%\end{itemize}

%Some initial stretch goals of the project were to construct an interactive
%interface to the resulting graphs to act as a visual aid for the results.

\section{Testing \& Evaluation}
With our dataset being so large, 6250 entries, it can
easily be split into training, development and testing with no
need for overlap. For this the data is split 10\% training,
10\% development and 80\% for testing.

Evaulation of the system was completed using the
Kendall rank correlation coefficient as is a statistic
used to measure the ordinal association between two
measured quantities. This was easy be applied to our results
by compairing the systems estimated orderings of events
with the label associated with the event from the data.

\chapter{Background}
%NLU as a subjext area
The work undertaken in this project settles nicely within the research space of Natural Language Understanding (NLU).
NLU deals with machine reading comprehension, the problem of dissecting natural language inputs into usable features,
and is considered an AI-hard problem.

Reading comprehension is the ability to read, process and understand text.  
There are two levels of reading comprehension, shallow (low-level) and deep (high-level).
Shallow comprehension involves structural and phonemic recognition along with the processing of sentence and  word
structure.
Deep comprehension involves semantic processing, which happens when we encode the meaning of a word and relate it
to similar words ( See word embedding).
This levels system of comprehension was first proposed by  Fergus I. M. Craik and Robert S. Lockhart \cite{wagner2009beyond}.

One of the earliest known attempts at NLU by a computer \cite{russell1995modern}. Since then, NLU has become an
umbrella term for a diverse range of problems and applications. These applications range from simple tasks, such as
issuing simple commands to robots, to highly complex tasks such as full comprehension of poetry passages.
Most problems within NLU fall somewhere between these extremes and will implement one of the types of comprehension
depending on the task.

Reading comprehension within these tasks is import as these application need not only understand the literal content
being passed to them, but also the contextual meaning within to better understand the nuances of the content.
The idea being that machine reading will be able to provide contextual information to allow better task performance when
involving unstructured data.

Unstructured data is data that has no pre-defined data model or manner. This type of data is
commonly seen in NLU with text mining and natural language rarely forming to any cohesive model \cite{feldman2007text}.
From the data sources typically used with NLU (news articles or free-form speech and text) the unstructured format of
the data is beneficial as structured data typically has little to no subtler meaning \cite{}.
%Bit more about reading comprehension is CS

In recent years there has been a commercial interest in NLU, mostly focusing on news-gathering, text categorising and
content-analysis. These systems vary vastly in terms of difficulty and use of comprehension. 

\chapter{Related Work \& Motivation}
%Papers I have read and why we are doing this
\section{Motivation}
\section{Related Work}


\chapter{Wikipedia As a Data Source}
%Intro to the data set
Wikipedia was chosen as the external data source for this project as
due to the nature of the events in our dataset, historical points of note,
a source with a wide and deep range of articles was required.
Ad wikipedia as over 5million articles \cite{} it is reasonable to assume
that most events will exist within its pages.

\section{Text Retrieval}
Using Wikipedia's article retrieval API, gathering the data from
the site is trivial. Choosing what to extract is, however, not
so straightforward.
\subsubsection{OpenIE}
Stanfords natural language processing (NLP) suite,
a natural language analysis toolkit,
was selected to aid in extraction of features from the dataset.
Using this toolkit, the Open Information Extraction (OpenIE) from
The University of Washington is used to extract Subjects, Objects
and their relations from our event titles.

OpenIE was chosen as it uses various extraction techniqes, such as ``noun-mediated extraction'',
to find the relations within a sentence. OpenIE can also deal with much more than simple relations,
it can look at positive or negative assertion within these relations which may be benefitial to our
solution. It was these details coupled with OpenIe's ability to build compound relationships that
made it the obvious choice for extracting the key information from out data points.

%\begin{figure}
%  \ref{fig:extraction}
%  \label{Example of Noun-mediated extraction}
%  \end{figure}

\begin{figure}
  \centering
  \ref{fig:compound-relations}
  \includegraphics[scale=0.5]{openie-train.png}[H]
  \caption{Example of a compund relationship}%\cite{OpenIE}
  \end{figure}
%As an example of OpenIE's extraction, the sentence:\\
%``The U.S. president Barack Obama gave his speech on Tuesday to thousands of people.''
%Will produce these potential binary relations\\
%``(Barack Obama, is the president of, the U.S.)''\\
%``(Barack Obama, gave, his speech)''\\
%``(Barack Obama, gave his speech, on Tuesday)''\\
%``(Barack Obama, gave his speech, to thousands of people)''\\
\subsubsection{Article Retrieval}
Using the Objects and Subjects generated by OpenIE we needed to get the relevent Wikipedia Articles.
To do this we used fuzzy searching of Wikipedia, through the use of their API, to get the articles relivent
for these Objects and Subjects.

Several different methods of extracting key sentences were experimented with.
These included:
\begin{enumerate}
\item Extracting only sentences that had a date within them
\item Extracting only sentences that had the other party in the relation within them
\item Extracting only sentences that had a date and the other party within them
\item Extracting paragaphs that referenced the other party
  \item extracting sentences that contained the object or the subject 
  \item extracting sentences that contained the object or the subject or referenced the action between the two 
  \end{enumerate}

%Through the experimentation, we found mixed results in terms of number of sentences retrieved and the relevency
%there in.

As we can see from Table \ref{table:retrieval}, experimentation yeilded mixed results.
With such a large range of average number of sentences retrieved and the relevency there in.
Relevency was measured manually using Cohen's Kappa \cite{}.

Cohen's kappa measures the agreement between two raters who each classify N items into C mutually exclusive categories
\begin{equation}
  \kappa = \frac{p_{0} - p_{e}}{1 - p_{e}}
  \end{equation}

\begin{table}[h]
\centering
\caption{Retrieval Methods and their results}
\label{table:retrieval}
\begin{tabular}{|l|l|l|}
  \hline
Method & Average \# Sentences Retrieved & Relevency \\
\hline
1      &                                &           \\
2      &                                &           \\
3      &                                &           \\
4      &                                &           \\
5      &                                &           \\
6      &                                &           \\        
\hline
\end{tabular}
\end{table}

From these results, method 6 was chosen as our context extraction method. While the
results from this method are not perfectly relevent and other improvments could be made,
time restirctions were a deciding factor in not persuing this experimentation further.

\section{Feature Representation}
Having extracted a series of sentences for each event in our data, we required a consise method
of representing these sentences.
\subsection{Bag-Of-Words}
The Bag-of-Words 
\subsection{Word Embedding}
\section{Potential Extntion of Data Retrieval}
\section{Potential Issues With Data Retrieved}
%While the ``Today in History'' dataset provides an excellent
%base for out experiments, the sentences alone
%(often between 5 and 15 words) do not contain enough
%information to extract relational information from.

%In order to build upon this data and procure data that
%would aid in our training it was decided to use wikipedia
%articles for aditional details.

%Wikipedia does contain articles on most any subject and so
%finding paragraphs about each event was simple enough. There
%were concerns however abgout the legitimacy of the content
%available, as any user has the ability to edit an article.
%This concern arose from the known conflict-of-interest editing
%that occurs on wikipedia.
%Read the wiki paper again anf pad this out
%\chapter{Date Extraction}

\chapter{Learning}
In order to produce results from the generated dates,
we has to choose the correct Machine Learning Techniques
\section{Aproach}

\section{Local Learning}
\subsection{Decision Trees}
\subsection{Random Forests}
\subsection{SVC}
\subsection{Regression}
\subsection{Classification Results}

\section{Global Learning}
\subsection{Structured Perceptron}
\subsection{Neural Networks}

\section{Results}

\chapter{Graphing}
\section{Traveling Salesman Problem}
\section{Encoding}
\subsection{Greedy}
\subsection{ILP}
\section{Results}

\chapter{Results}
\section{Results}
\section{Discussion}
\chapter{Conclusion}
\section{Further Work}
\section{Conclusion}
% use the following and \cite{} as above if you use BibTeX
% otherwise generate bibtem entries
\bibliographystyle{apalike}
\bibliography{./references.bib}
\end{document}
