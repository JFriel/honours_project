% to choose your degree
% please un-comment just one of the following
\documentclass[bsc,frontabs,twoside,singlespacing,parskip,deptreport]{infthesis}     % for BSc, BEng etc.
% \documentclass[minf,frontabs,twoside,singlespacing,parskip,deptreport]{infthesis}  % for MInf
\usepackage{cite}
\usepackage{graphicx}
\usepackage{float}
\usepackage{color}
\usepackage{xcolor}
\usepackage{ntheorem}   % for theorem-like environments
\usepackage{mdframed}   % for framing
\usepackage{csquotes}
\usepackage{amsmath}

\usepackage{tcolorbox}% http://ctan.org/pkg/tcolorbox
\tcbuselibrary{skins}
\usetikzlibrary{shadings}

\begin{document}

\title{Temporal Ordering of Historical Events using Contextual Data}

\author{James Robert Friel}
% to choose your course
% please un-comment just one of the following
\course{Artificial Intelligence and Computer Science}
%\course{Artificial Intelligence and Software Engineering}
%\course{Artificial Intelligence and Mathematics}
%\course{Artificial Intelligence and Psychology }   
%\course{Artificial Intelligence with Psychology }   
%\course{Linguistics and Artificial Intelligence}    
%\course{Computer Science}
%\course{Software Engineering}
%\course{Computer Science and Electronics}    
%\course{Electronics and Software Engineering}    
%\course{Computer Science and Management Science}    
%\course{Computer Science and Mathematics}
%\course{Computer Science and Physics}  
%\course{Computer Science and Statistics}    
 
% to choose your report type
% please un-comment just one of the following
%\project{Undergraduate Dissertation} % CS&E, E&SE, AI&L
%\project{Undergraduate Thesis} % AI%Psy
\project{4th Year Project Report}

\date{\today}

\abstract{}
%TODO


\maketitle

\section*{Acknowledgements}
Firstly I would like to thank my supervisor Shay Cohen for his guidance throughout the project.

I would also like to thank the ``School of Memeformatics'' group for assisting as a brain trust
throughout the project.

Thank you to Greggs Bakery for being there for me through thick and thin.

Lastly, thank you to Emily Wilson for her tireless proofreading and ability to put up with me.

\tableofcontents

%\pagenumbering{arabic}


\chapter{Introduction}
\section{The Problem}
Nominal data is not descriptive in nature. This lack of information makes it difficult to assign
a canonical ordering to.
The problem tackled within this dissertation is to test if the use of external data sources can
aid in the ordering of news article headlines.
We will be looking at the usefulness of Wikipedia as an external source in order to timeline
historical events.

Our event data comes from the ``Today in History'' dataset. Each of these data points consists
of an event and a date\\
$[$ ``Alaska becomes 49th State'', ``1959-01-01'' $]$


The problem tackled in this paper is based on the paper \cite{abend2015lexical} and several
techniques used are based from this paper. 

\section{Aims \& Objectives}

The aim of this project is to experiment with machine reading on external data sources to
discover if it is feasible to extract context from these sources to aid in the linear ordering
of historical events.

We aim to do this be experimenting with machine reading techniques to try and extract the most
relevant contextual information from our data sources.

With this information we intend to experiment with classification methods to optimise estimations
of local orderings before generating a graph from the returned data.

Using the graph generated from our classifier, we will attempt to find the optimum maximum spanning path
through every node. This path will be our estimated ordering of history.

We can use some evaluation technique to see how accurate our results are, compare those to not using
external data sources and to a human rate.


\section{Testing \& Evaluation}
With our dataset being so large, 6250 entries, it can
easily be split into training, development and testing with no
need for overlap. For this the data is split 10\% training,
10\% development and 80\% for testing.

Evaluation of the system was completed using the
Kendall rank correlation coefficient as is a statistic
used to measure the ordinal association between two
measured quantities. This was easy be applied to our results
by comparing the systems estimated orderings of events
with the label associated with the event from the data.

\section{Implementation}
\section{Contributions}

\chapter{Background \& Related Work}


\section{Relation Extraction}
Dependency parsing is the core of relation extraction. Focusing on the dependencies between words allows for the relation of these words to become apparent.
While there are numerous types of dependency representations, relation extraction is mostly focused around semantic dependencies \cite{mcclosky2011event}.
Semantic dependencies are understood in terms of predicates and arguments.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{semantic-dependancy.png}
  \caption{Example Semantic Dependency}
  \label{fig:semantic-dependancy}
\end{figure}

In the example semantic dependency in figure \ref{fig:semantic-dependancy} the two arguments \textit{John} and \textit{Football} are dependent
on the predicate \textit{likes}. This semantic dependency is the basis for relation extraction.


Historically, dependency parsing has consisted of only highly local models that extract each event and argument independently.
Modern techniques are building upon this to build systems that can handle more complex arguments. These systems use a tree of
event-argument relations represented in a re-ranking dependency [parser \cite{mcclosky2011event}.
This type of system captures both flat relations, such as in figure \ref{fig:semantic-dependancy}, but also nested relations.
We see an example of these nested relations in figure \ref{fig:nested-relation}. These nested relation allow for more context to be drawn from
the source text.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{nested-relation.png}
  \caption{Example Nested Relation}
  \label{fig:nested-relation}
\end{figure}


\section{Machine Reading}
The core aim of this project is to investigate the usefulness of Wikipedia article content is beneficial to total ordering of events.
In order to do this we must perform machine reading on said articles.
Machine reading is the core problem of Natural Language Understanding (NLU).
NLU deals with machine reading comprehension, the problem of dissecting natural language inputs into usable features,
and is considered an AI-hard problem.

Reading comprehension is the ability to read, process and understand text.  
There are two levels of reading comprehension, shallow (low-level) and deep (high-level).
Shallow comprehension involves structural and phonemic recognition along with the processing of sentence and  word
structure.
Deep comprehension involves semantic processing, which happens when we encode the meaning of a word and relate it
to similar words, see section \ref{sec:word-embedding}.
This levels system of comprehension was first proposed by  Fergus I. M. Craik and Robert S. Lockhart \cite{wagner2009beyond}.

One of the earliest known attempts at NLU by a computer \cite{russell1995modern}. Since then, NLU has become an
umbrella term for a diverse range of problems and applications. These applications range from simple tasks, such as
issuing simple commands to robots, to highly complex tasks such as full comprehension of poetry passages.
Most problems within NLU fall somewhere between these extremes and will implement one of the types of comprehension
depending on the task.

Reading comprehension within these tasks is import as these application need not only understand the literal content
being passed to them, but also the contextual meaning within to better understand the nuances of the content.
The idea being that machine reading will be able to provide contextual information to allow better task performance when
involving unstructured data.

Unstructured data is data that has no pre-defined data model or manner. This type of data is
commonly seen in NLU with text mining and natural language rarely forming to any cohesive model \cite{feldman2007text}.
From the data sources typically used with NLU (news articles or free-form speech and text) the unstructured format of
the data is beneficial as structured data typically has little to no subtler meaning \cite{}.
%Bit more about reading comprehension is CS

In recent years there has been a commercial interest in NLU, mostly focusing on news-gathering, text categorising and
content-analysis. These systems vary vastly in terms of difficulty and use of comprehension. 


\section{Related Work}
In recent years there has been a growing research into temporal ordering.
A number of different papers have been published on machine-learning approaches to temporal ordering events.
Although there have been various different approaches to this problem, the most common and successful approaches typically
use unsupervised learning techniques. These works often use pattern-based approaches and manually crafted rules \cite{chklovski2004mining}.  

\cite{chambers2009unsupervised}addresses unsupervised learning of event relations. Their evaluation
scenarios deals with binary classification related to event ordering and aims to distin-
guish ordered set of events from random permutations. Their research into argument representation is beneficial base
knowledge for our research into event extraction.

The paper by \cite{abend2015lexical} addresses the use of
edge-factor models in temporal ordering. The classification task we aim to undertake is similar to that of \cite{abend2015lexical}
but with significantly larger set of events to order. \cite{abend2015lexical} also use a variation of the Hamiltonian path
to complete an ILP ordering of their events.

\cite{mani2006machine} discuss the use of hand written rules and lexical rules for event tagging. While the methods discussed in this paper
do not take into account dependencies between pairs of events, we see an alternative that does encompass
these dependencies in \cite{schapire1998learning}.  

%Should probably expand on each paper, advantages disadvantages etc

\chapter{Methodology}
%Papers I have read and why we are doing this
%\section{Motivation}
%\section{Related Work}


%\section{The Data}%%Potential Chapter name change
%Intro to the data set
\section{Today In History Dataset}
The Today In History data set we are working with consists of 6225 entries. Each entry consisting
of a  brief event title and the date in the form ``YYY-MM-DD''.
An example entry is ``1934-01-01, Alcatraz officially becomes a Federal Prison''.
This data comes from Wikipedia's Today In History features. This is a feature that provides events from
throughout history that occurred upon the date of visit. The data is a collection of these events and has
an even distribution of events across the year(mean 17.05, average XXX events per day).


This dataset was chosen as the events within are all objective historical events. This is beneficial as it minimised
any ambiguity when selecting Wikipedia articles to process. This, coupled with date labels ,already associated with
each event, made the dataset very favourable.


\section{Choice of External Data Source}
Wikipedia was chosen as the external data source for this project.
This is due to the nature of the events in our dataset, historical points of note,
needing a source with a wide and deep range of articles.
As Wikipedia has over 5 million English language articles \cite{wikipedaisize} and our original data
is taken from a Wikipedia outlet, it is reasonable to assume
that most events will exist within its pages.


Wikipedia is also written in an reasonably unbiased, procedural manner which makes comprehension of events much
easier than that of raw, unstructured data.
It is the key sentences within articles that we will use to estimate a timeline.

\section{Text Retrieval}
Using Wikipedia's article retrieval API \cite{wikipediaAPI}, the task of gathering article content
from keywords in its title is trivial.

Choosing what information is contextually relevant to the subject is, however, not so straightforward.
Along with looking at how to extract the key entities and actions from our 
headlines, we must also experiment with methods of extracting contextually relevant information from
retrieved articles related to the event headline.

\subsubsection{Information Extraction}
Stanford's natural language processing (NLP) suite,
a natural language analysis toolkit,
was selected to aid in extraction of features from the dataset.
Using this toolkit, the Open Information Extraction (OpenIE) from
The University of Washington is used to extract Subjects, Objects
and their relations from our event titles \cite{schmitz2012open}.


OpenIE implements a number of extraction techniques, such as ``noun-mediated extraction'' and ``n-ary extraction''.
We have seen through experimentation with the various extraction techniques that no one method provides
usable relations for all inputs. Using OpenIE provides us with the ability to use multiple techniques and
use the best fitting results.

This system provides better relationship extraction than other systems available. Systems such as
ReVerb or WOE suffer from a few key weaknesses. These systems only provide verb mediated relation extraction \cite{schmitz2012open}. This is problematic as sentences such as

``EXAMPLE''

would not have the relation XXX extracted due to the structure of the relation lacking a verb.
These systems are also not built to use context for relation extraction and thus extract non-factual relations \cite{schmitz2012open}.
It is for these reasons that ReVerb and WOE were not chosen as our information extraction technique.


Along with OpenIE's ability to seamlessly use multiple extraction techniques to find the best available
relations for each event, it has numerous other properties that are beneficial to our project.
OpenIE is also useful for our purposes as it does not require a schema specified in advance.
This is useful as we do not know the structure of the content that will be looked at.
This will allow the system to extract
more complex relationships, such as those seen in Figure \ref{fig:compound-relations}, if such
a relationship can be found.


\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4]{openie-train.png}
  \caption{Example of a compound relationship \cite{OpenIE}}
  \ref{fig:compound-relations}
  \end{figure}

OpenIE can also deal more complex relations  than the simple relations we have see.
It can look at positive or negative assertion within these relations.
This extra information may be beneficial to our solution as particular lenses are often applied to historical events.
It was these properties that make OpenIE the optimal system to extract the key relations from our events.


\subsection{Article Retrieval}
Using the objects and subjects generated by OpenIE we needed to get the relevant Wikipedia Articles.
To do this we used fuzzy searching of Wikipedia, through the use of their API, to get the articles relevant
for these objects and subjects.

Several different methods of extracting key sentences were experimented with.
These included:
\begin{enumerate}
  \item Extraction of only sentences that had a date within them
  \item Extraction of only sentences that had the other party in the relation within them
  \item Extraction of only sentences that had a date and the other party within them
  \item Extraction of paragraphs that referenced the other party
  \item Extraction of sentences that contained the object or the subject 
  \item Extraction of sentences that contained the object or the subject or referenced the action between the two 
\end{enumerate}


As seen in Table \ref{table:retrieval}, experimentation yielded mixed results.
The results show a large range of average number of sentences retrieved and varying degrees of relevancy.
Relevancy was measured manually using Cohen's Kappa (see Section \ref{sec:cohen})\cite{wood2007understanding}.

%Cohen's kappa measures the inter-rater agreement between two raters
%who each classify N items into C mutually exclusive categories.
%
%Cohen's Kappa is defined as:\\
%\begin{equation}
%  \kappa = \frac{p_{0} - p_{e}}{1 - p_{e}}\nonumber
%\end{equation}
%Where $p_0$ is the relative observed agreement between raters and $p_e$ is the hypothetical probability
%of a chance agreement using the observed data to calculate the probabilities of each observer randomly saying each category.
%If the raters are in complete agreement then $\kappa$ = 1.
%If there is no agreement among the raters, other than that expected by $p_e$, $\kappa \leq 0$.
%
%Cohen's Kappa is thought to be a more robust measure than a simple percentage agreement.
%This is due to it taking into account the possibility of agreement occurring by chance.
%
%
Given that we wish to categorise our retrieved sentences into either related to the event, or not related,
we only have two possible categories to assign each of our N sentences to.

\begin{table}[h]
\centering
\label{table:retrieval}
\begin{tabular}{|c|c|c|}
  \hline
Method & Average \# Sentences Retrieved & Relevancy \\
\hline
1      & 16                             &   -0.875  \\
2      & 2                              &   -1      \\
3      & 0.04                           &   0.5     \\
4      & 20                             &   -0.4    \\
5      & 27                             & -0.1\.{1}\\
6      & 32                             & 0.44\.{4}\\        
\hline
\end{tabular}
\caption{Retrieval Methods and their results}
\end{table}

From these results, method six was chosen as our context extraction method.
While it does not return the most relevant results, extraction method 3 did not provide
a high enough number of sentences on average to build features from.


Despite extraction method six not generating a perfect relevancy, under the time constraints
of the project, it  was decided to move forward with the project rather than to incorporate more
complex solutions to improve relevancy.

%Link above to below section 
\subsection{Potential Issues With Data Retrieved}\label{sec:dataIssues}
Wikipedia was a good choice for our external data source due its large catalogue of events.
This provided us with more than sufficient data to gather from.
While we have seen all of these positive attributes of Wikipedia, there are certain issues that may have to be addressed
that, if the content domain was to be altered, may cause an issue.

As Wikipedia is community managed, anybody can edit any page. This has been known to cause 'edit wars' over users
uploading biased information about subjects \cite{}.
While in our current domain of long-term history, the facts and timelines are suitably coherent, if we were to apply this project
to a much more current or hotly contested subject - such as the OJ Simpson trial, or recent presidential election - Wikipedia
may not be the most appropriate source of information due to these bias edit wars.
%Example


\section{Feature Extraction}\label{sec:representation}
Having extracted a series of sentences for each event in our data, we required a concise method
of representing these sentences. It is standard in NLP and ML domains to represent features as vectors.
Several different techniques were discussed, but ultimately due to time constraints only two methods were
experimented with: Bag-Of-Words and Word Embedding.
\subsection{Bag-Of-Words}
The Bag-of-Words model is a simplifying representation of text.
In this model, we take a sentence, or multiple sentences, and represent them as a bag(multiset) of its words.
A multiset is similar to set, but can allow multiple of the same item within.
For example, \{$\alpha,\beta$\} and \{$\alpha,\beta,\alpha$\} are the same set, but individual multisets.
This technique disregards grammar and order, but keeps multiplicity. It is a common technique in Computer vision
and other learning situations \cite{sivic2009efficient}.

%Need to talk a bit more abput how we are making tuples of every event set
For our project, we implemented Bag-of-Words by using the relevant sentences for both of our events as a feature.
We chose to represent our multisets are counts over the vocabulary so as to build numerical features to easily
use the data with classifiers.
%\newpage
\begin{tcolorbox}[width=\textwidth,
                  %%enhanced,
                  %%frame hidden,
                  interior hidden,
                  boxsep=10pt,
                  left=0pt,
                  right=0pt,
                  top=2pt,
                  ]%%
  Given the sentences \vspace{0.5em}\\
  \begin{center}``John likes to watch movies. Mary likes movies too.''\\
  ``John also likes to watch football games.''\vspace{1em}\\
   \end{center}
    Based on these two text documents, a vocabulary list is constructed as follows:\vspace{1em}\\
    \begin{center}
  [``John'',``likes'',``to'',``watch'',``movies'',``also'',``football'',``games'',``Mary'',``too'']\vspace{1em}\\
\end{center}
    From this list a vector is constructed of the occurrences
    of each word across the sentences\vspace{1em}\\
 \begin{center} 
  Result: [2,3,2,2,2,1,1,1,1,1]
\end{center}
\end{tcolorbox}

As Bag-Of-Words only looks at single words at time and the contextual data may be lost through this method, we looked at
alternative methods to improve on this.

\subsection{N-Grams}
When experimenting with N-Grams, we aimed to try and capture some of the meaning of phrases that appeared in the gathered
sentences.
This was achieved by looking for phrases and sequences of words that recurred throughout the sentences - under the assumption
that what reoccurs must be important. Due to the wide variety in the structure of the data we were seeing,
simple 2- or 3-Grams provided little to no use, returning junk phrases if anything at all.


%Potentially add stuff about skip-grams
%With the failure of N-Grams at this problem, we attempted to implement Skip-Grams from the paper \cite{guthrie2006closer}.
%By missing out 1 or 2 words in the middle of key words, skip-grams aim to extract improved phrases from the data by
%'skipping' filler words, leaving only the important words for extraction.

%We found that the use of skip-grams
%Something about why this wasn't better.

%This representation worked well as it it provides a numerical representation of the cross-section of our two articles
%but does remove some of the contextual data we might receive.

\subsection{Word Embedding} \label{sec:word-embedding}
Word Embedding was also considered as this feature learning technique maps words or phrases to vectors of real numbers.
With advances in this method from Google in the past few years, with the use of neural network architectures, they have become
popular as methods to perform natural language processing (NLP) such as syntax parsing \cite{socher2013parsing}.

Using Gensim's Doc2Vec models \cite{rehurek_lrec}, we used this system to build thought vectors of our sentences.
Thought vectors are vectors made of whole sentence or documents, typically used in machine translation \cite{deeplearning4j}.
It was decided to use thought vectors as they correlate words with labels rather than word with words (which is done by bag-of-words).
This would, in theory, provide better better representation of the words relating to ordering label, providing better results.

%Example of word embedding and such


From our experimentation, we discovered that thought vectors produced an accuracy 54\% less than that when
using bag-of-words. While this went against our hypothesis of using this technique we believe
the main reason that word embedding did not perform as we expected was due to imperfect sentence retrieval causing noise in
the features.

\section{Classifiers}\label{sec:classifiers}
\subsection{Decision Trees}
Decision trees were considered as they are a non-parametric, supervised learning method that care commonly used for
classification and regression.
They are based on the idea of asking simple yes/no questions. The algorithm is set up so as to be a binary tree with
each node being a question and each leaf a decision.

By inferring decision rules from data features, the system creates a model that predicts the value of a target variable,

These attributes were promising for our experiments as they fit with our data and generate
classifications of test data along with probabilities of these estimations.

%Some issues found with decision trees were that the probabilities generated were too narrowly distributed
%to infer any confidence in the paths generated. Due to this, it was decided to explore other methods of
%classification.
%\subsection{Random Forests}

\subsection{Support Vector Machines}
Similarly to Decision trees, Support Vector Machines (SVMs) provide supervised learning models based on
associated learning algorithms for classification and regression.

Given a set of labelled training data, an SVM will map this into a 3-D vector space.
When an unlabelled data point is fed into the system, the SVM fits the new data into the vector space and
assigns  it to the closest vectors class.

%This proved beneficial as the results generated were better than the decision trees \ref{table:local-learning}.
%SVMs also provided much better classification probability, which is favourable for path generation.
%\subsection{Linear Regression}
% To Be Completed in due course
%\vspace{4em}
\subsection{Logistic Regression}
Logistic regression was also experimented with as it utilises a binary dependent variable - that being where it
can only take one two values, ``1'' or ``0'' in our case to represent correct ordering.


The logistic regression model estimates the probability of a binary response based on one or more independent features.
This is beneficial as along with providing classification estimates, the model can produce probabilities for its
decision. These probabilities can allow us to use weighting techniques to improve our path building.

\subsection{Perceptron}
The Perceptron is a common algorithm used for large-scale learning.
It is a type of linear classifier that bases predictions on a linear predictor function that combines a set of optimised weights
and the input feature vector.
It is primarily used for supervised learning of binary classification.
This is something that will fit well with our data as the perceptron decides whether an input vector
corresponds to a specific class \cite{freund1999large}.

The percepron model has several attributes that will aid in improving the accuracy of our results.
These include:
\begin{itemize}
\item It does not require a learning rate.
\item It is not regularised (penalised).
\item It updates its model only on mistakes
\end{itemize}

Perceptrons also allow for retraining of the model when newer data is added, this allows the perceptron to
itterativly improve itself. An example of this improvement can be seen in figure \ref{perceptronExample}.
\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.5]{PerceptronExample.png}
  \ref{fig:perceptronExample}
  \caption{A diagram showing a perceptron updating its linear boundary as more training examples are added. \protect{\cite{goodspeed_2015}}}
\end{figure}


It is these attributes that made the Perceptron a suitable model to experiment with.
%For our experimentation, we trained the Perceptron model using  10 iterations with no regularisation term.
\subsection{Multilayer Perceptron}
Having discussed the use of Perceptrons, the next logical method to experiment with is multilayer perceptrons (MLP).
A Multilayer Perceptron is an feed-forward artificial neural network that maps the set of inputs
onto a set of appropriate outputs.
In figure \ref{feg:mlp} and example layer of an MLP.
An MLP consists of layers of  nodes in the form of a directed graph, with each layer connected to the next.
Each node, except the input node, is a neuron (processing element) with an activation function \cite{}.%need a ref here

\begin{figure}
  \centering
  \includegraphics[]{mlp-example.png}
  \caption{An Example one hidden layer MLP \cite{scikit-learn}}
  \label{fig:mlp}
\end{figure}

An activation function is a function that maps the weighted inputs of each processing element to an output.
MLPs are trained using backpropigation.

\section{Evaluation}
Throughout our experiments, there are several aspects that need to be evaluated.
The aspects that are required to be measured are:
\begin{itemize}
  \item Retrieved Sentence Relevency
  \item Accuracy of Classifier
  \item Correlation of Predicted and True Path
\end{itemize}

There are multiple different staticstic that were considered for each of these problems.
\subsection{Choice of Sentence Revelency Metric}\label{sec:cohen}
In order to select a method to extract relevent sentences from Wikipedia, a statistic that measured the agreement
of relevency. There are two main statistics for measuring inter-rater reliability: Cohen's Kappa and Fleiss' Kappa \cite{}.
These two statistics measure the agreement between reviewers. The main difference between Cohen's and Fleiss' Kappa is that
Cohen's Kappa only works for two reviewers, where as Fleiss' Kappa works for any number of fixed reviewers \cite{}.
Given the short time frame of this project and the length of data reviewed, it was decided to use Cohen's Kappa in order
to find an agreement between two reviewers selection of relevent sentences gathered from Wikipedia.

Cohen's kappa measures the inter-rater agreement between two raters
who each classify N items into C mutually exclusive categories.

Cohen's Kappa is defined as:\\
\begin{equation}
  \kappa = \frac{p_{0} - p_{e}}{1 - p_{e}}\nonumber
\end{equation}

Where $p_0$ is the relative observed agreement between raters and $p_e$ is the hypothetical probability
of a chance agreement using the observed data to calculate the probabilities of each observer randomly saying each category \cite{}.

If the raters are in complete agreement then $\kappa$ = 1.
If there is no agreement among the raters, other than that expected by $p_e$, $\kappa \leq 0$.

Both statistics are thought to be a more robust measure than a simple percentage agreement.
This is due to it taking into account the possibility of agreement occurring by chance.

\subsection{Choice Of Correlation Coefficient}\label{sec:kendall}

There are various different correlation coefficents that could be used to evaluate our generated pathways.
Some popular coefficients include:
\begin{itemize}
\item Pearsons Correlation Coefficient ( Pearson's r) - a measure of the strength and direction of the linear relationship between two variables
\item Intraclass Correlation - describes how strongly units in the same group resemble each other
\item Kendall Rank Correlation Coefficient -  measure of the portion of ranks that match between two data sets
\end{itemize}

For evaluation of our ordering, we decided to use the Kendall rank correlation coefficient (Tau coefficient).
This statistic was chosen as it can be used to measure the ordinal association between two measured quantities.
This allows us to evaluate what proportion of our ordering was correct, giving a truer representation of accuracy
compaired to Pearson's r or Interclass correlation \cite{}.
Pearson's r was not chosen as it measures the linear corelation between two variables\cite{}.
This would not produce an accurate representation of the system as a single misaligned event would reduce the
correlation drastically.
Kendall's Tau was also prefered over Interclass correlation as ...\cite{}


The Tau coefficient is defined as
\begin{eqnarray}
  \tau=\frac{(\text{number of concordant pairs})-(\text{number of discordant pairs})}{n(n-1)/2}\nonumber
\end{eqnarray}\cite{abdi2007kendall}

The Tau coefficient was used as it measures results in terms of agreement in the range $-1 \leq \tau \geq 1$.
This allows us to show the agreement of our orderings, with 1 being a perfect agreement of the ordering, -1 being a perfect disagreement of the ordering and 0 claiming
total independence.


\section{Summary}
TODO

\chapter{Experiments}
\section{Classification}
In order to investigate if retrieved sentences aided in the  ordering of linear event we experimented with various
machine learning techniques to try and learn how to order events by sentences.
We hoped to see that the features generated from these 'important' sentences provided a suitable training model
to improve orderings against a random assignment baseline. We also hoped to see that the use of these features vastly
improved over the use of titles alone.


\subsection{Approach}
In order to be able to train a classifier on our data, a method of giving each feature a label must be established.

 Given that each event in our data set is of the form
  \begin{eqnarray}
  (t_{i},d_{i}) \hspace{0.5em} \text{for} \hspace{0.5em} i \hspace{0.2em}\epsilon \hspace{0.2em} [M]\nonumber
    \end{eqnarray}
    where t is the title, d is the associated date and M is the original data set,

    Using the Wikipedia extraction techniques discussed in section \ref{sec:retrieval} we constructed a new data set
    \begin{eqnarray}
      \{(t_{i},s_{i},d_{j})\} \hspace{0.5em} \text{for} \hspace{0.5em}i,j  \hspace{0.2em}\epsilon \hspace{0.2em}[M]\nonumber
    \end{eqnarray}
    Where $s_i$ is the sentences retrieved from Wikipedia for title $t_i$. 
    This formed the basis of our data to generate features.

    From this we built a new data set
    \begin{eqnarray}
      \{(t_{i},s_{i},t_{j},s_{j},b_{ij})\} \hspace{0.5em} \text{for} \hspace{0.5em} i,j \hspace{0.2em}\epsilon \hspace{0.2em}[M]\nonumber
    \end{eqnarray}
    where $b_{ij} = [y_{i} > y_{j}]$ indicating which event came first.\\
    We used this value as the label for our training data and to evaluate our results against.
    A similar data set was constructed using only the article headlines.
    
    From each on the entries in this new data set, we use the feature representation techniques discussed in section \ref{sec:representation} to build numerical
    features to train our classifier with.

Along with building a tuple event dataset, we experimented with triple event entries.
\begin{eqnarray}
      \{(t_{i},s_{i},t_{j},s_{j},t_{k},s_{k},b_{ijk})\} \hspace{0.5em} \text{for} \hspace{0.5em} i,j,k \hspace{0.2em}\epsilon \hspace{0.2em}[M]\nonumber
\end{eqnarray}

Where $b_{ijk} = [y_i > y_j > y_k]$ indicating if the ordering is correct for these events.

While we did not anticipate that increasing the dependency length of each entry would improve accuracy, it was investigated
for the sake of completeness. We anticipated that using more events per feature would diminish the accuracy of the system as
...FIND A PAPER

As we expected, increasing the number of events associated with each training point greatly decreased the accuracy of results in all cases.
Due to this, we will not focus on discussing these experiments,but instead focus on the results gathered from the tuple dataset.

We split our dataset into 80\% for training, 10\% for development and 10\% for testing.
This beaks our 6226 entry dataset into 4982,622,622 for the respective split.
The allotment of the entries into each category was done randomly.

%For each of the techniques outlines below, we followed a similar procedure.
Experimentation was done using the development set to optimise the parameters to give us the most optimal results
when finally tested on the testing data.
For each classifier, we feed in the training features and their associated label. These features will
be either the features generated from titles alone or with the sentences retrieved from Wikipedia.

Once training of the model has completed, we feed in a new data point without a class label.
The system will return a predicted class label for the new data and a probability score for its categorisation.

Each of these data points is similar to that of the training data
 \begin{eqnarray}
   \{(t_{i},s_{i},t_{j},s}{j})\} \hspace{0.5em} \text{for} \hspace{0.5em} i,j \hspace{0.2em}\epsilon \hspace{0.2em}[M]\nonumber
 \end{eqnarray}

 The classification returned for each data point is of the same style as that of the training data.
 %With these new data point classification we can begin to look at finding the optimal path through all of the nodes.
 %We will discuss the methods used in Chapter \ref{chapter:graphing}.
 Each of the classification methods discussed in section \ref{sec:classifiers} required experimentation to optimise the
 parameters. Once optimisation of the parameters has occurred, we saw the best results from our classifiers.

 \subsection{Decision Tree}
 As Decision Trees infer decision rules from the training data provided to it, there are very few parameters to
 experiment with. With our system being a binary classification system, there is not need to offset the class weightings.
 From this we did not require any parameters to be used when training the classification tree.
 
 \subsection{Support Vector Model}
 SVMs have the option of many different parameters to be tuned in order to optimise the results.
 In out implementation of an SVM we had the following parameters to experiment with:
 \begin{itemize}
 \item Penalty parameter of the error term
 \item The kernel type to be used in the algorithm
 \end{itemize}

 From our experimentation with our development data, we found the optimal penalty parameter to be 1.0.
 This parameter indicates to the model how much you want to avoid classification of each example.
 Setting our penalty to 1.0 will severely penalise the model for classification and leads to model producing
 better results.

 SVMs use a kernel to turn a linear model into a mon-linear model. This is benefits as it allows the system
 to operate in high-dimensionality, implicit feature space without having to compute the coordinates of the feature
 in space \cite{hofmann2008kernel}.
 From our experimentation the Radial Based Function (RBF) kernel was chosen as it helps SVMs scale to large numbers of
 training samples which they do not typically perform well with.

 The RBF kernel for two same vectors, x and \'{x}, is defined as:\\
 \begin{eqnarray}
   K_{RBF} (x, \'{x}) = \text{exp}[ \frac{||x =\'{x}||^2}{2\sigma^2}] }\nonumber
 \end{eqnarray}
\cite{hofmann2008kernel}
 This kernel can project into infinite dimensions and therefor allows for easy computation of new coordinated given that
 we have a high number of training samples.
 
 \subsection{Logistic Regression}\label{sec:log-reg}
 When using the Logistic Regression model, there were only a few parameters that were of interest to us:
 \begin{itemize}
   \item Type of penalty
   \item Type of solver
 \end{itemize}

 There are two main types of penalty's that can be applied to Logistic Regression models, L1 and L2 loss functions.
 L1, also know as  the least absolute deviations, is `a loss function that minimises the sum of of the absolute
 differences (S) between the target ($Y_i$) and the estimated values ($f(x_{i})$) \cite{l1l2}.
 \begin{eqnarray}
   S = \sum_{i=1}^{N} | Y_i - f(x_i)|\nonumber
   \end{eqnarray}
 
 L2 is also know as least square error. It minimises the sum of the square of differences (S) between
 the target ($Y_i$) and the estimated values ($f(x_{i})$) \cite{l1l2}.
 \begin{eqnarray}
   S = \sum_{i=1}^{N} ( Y_i - f(x_i))^2\nonumber
   \end{eqnarray}

 L2 was chosen as it will always provide a single, stable solution where as L1 can return multiple solutions
 which is of no use for our system.

 There are several different solvers that can be effectively used with a Logistic Regression model.
 For our system, we chose to use the LIBLINEAR solver as it supports L2, provides probabilities
 for Logistic Regression and is a linear classifier for large data sets\cite{fan2008liblinear}.
 
 \subsection{Perceptron}
 During our experimentation with Perceptron parameters, we discovered that giving the system nine epochs (passes
 over the training data) and shuffling the training data on each iteration provided results that maximised the
 estimations. For the same reasons stated in section \ref{sec:log-reg}, we use an L2 penalty with our perceptron
 model.
 
 \subsection{Multilayer Perceptron}
 In order to effectively use an MLP for our data we experimented with the various parameters.
 \begin{itemize}
 \item Number of hidden layers - the number of neurons per layer
 \item Type of activation function
 \item Type of solver
 \item L2 penalty
 \item Maximum number of iterations
 \end{itemize}
 
 After running experiments on  small subsets of our data, we found the optimal number of hidden layers to be 100. 
 This aids the model in transforming the inputs into something usable for the output layer.
 
 Having looked at logistic and identity activation functions, we settles on the rectifier activation function.
 The rectifier activation function is the most popular activation function \cite{lecun2015deep} and if defined as
 \begin{eqnarray}
   f(x) = \text{max}(0,x)\nonumber\\
   \text{where x is the output of a neuron}\nonumber
 \end{eqnarray}
 The rectifier function was chosen as it is more effective than its available counterparts\cite{glorot2011deep}.

 In order to solve the weight optimisation, we used the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm.
 This algorithm was chosen as it is an iterative model for non-linear optimisation.
 %Some more stuff about it

 An L2 penalty 0.00005 was chosen to aid our model beyond the training data.
 We chose the value of 0.00005 so as to ...

 The last parameter we has to experiment with was the number of epochs.
 We wanted each layer to reach a convergence, so the parameter was set to 100.
 100 layers was chosen as during experimentation with subsets of the data improvements were
 insignificant showing that the solver was near convergence.

 \section{Path Finding} \label{chapter:graphing}
From our classifier, we have constructed a directional ordering between every two events.
By placing each of our events in a node and placing each individual ordering on an edge, we constructed
a digraph of the data.
A digraph is an ordered pair G = (A,V) where\cite{bang2008digraphs}
\begin{itemize}
  \item V is the set of nodes
  \item A is a set of ordered pairs of vertices
\end{itemize}
An example of such a graph can be seen in figure \ref{fig:digraph}.

It is with this new graph that we began to experiment with methods to construct the most probably path through every
node in the graph.


\begin{figure}
  \centering
  \includegraphics[scale=0.5]{ExampleDigraph.png}
  \caption{A small example of our results places in a digraph}
  \label{fig:digraph}
 \end{figure}

\subsection{Travelling Salesman Problem}
The Travelling Salesman Problem (TSP) is a classic NP-hard algorithm problem\cite{junger1995traveling}.%something about NP hard problems
It is defined as
\begin{center}
\enquote{A travelling salesman wants to visit each of a set of towns exactly once, starting
from and returning to his home town. One of his problems is to find the shortest
such trip. - \cite{junger1995traveling}}
\end{center}

While the TSP algorithm is still unsolved, we fortunately only had to solve a a modified version of the problem.\\

\begin{center}
  \enquote{Given a graph with directed edges, find the optimal path from some starting node through all other nodes in
  the graph.}
\end{center}

These restriction on the graph make finding an optimal path through the graph much simpler.


\subsection{Pathing Algorithms}
There have been numerous algorithms developed for the task of finding the shortest and longest paths through various types of graphs.
As we require a total traversal of our graph, several of these algorithms were eliminated.

In order to accommodate many shortest-path algorithms, a new graph was constructed with edge weights between nodes being inverted,
$\frac{1}{W}$ where W is the edge weight. This allowed paths that look for the cheapest path to be usable as they will traverse the
true most expensive path, which will encompass all nodes.

There are two main types of solutions to optimal pathing: Greedy and Integer Linear Programming.
We dissed the option available in each type of solution and the benefits there in.

\subsection{Greedy Solution}
A greedy algorithm is an algorithm that follows the problem solving heuristic of making locally optimal choices at each stage with the aim
of finding a global optimum \cite{black2004dictionary}.
While a greedy strategy does not in general produce a global optimum, it may produce locally optimum solutions that approximate a globally optimum solution.


The greedy strategy for our variation of the travelling salesman problem is the following heuristic:
``At each stage, visit an unvisited city nearest to the current city.''
Where nearest is defined as the lowest edge-weight.


%Need some beam search refs
The greedy algorithm selected for our problem was the beam search algorithm.
It was chosen as it follows the greedy heuristic and also stores partial orderings, allowing us to prune solutions that do not cover all nodes.
An example of this pruning ability can be seen in figure \ref{fig:beam-prune}.

\begin{figure}
  \includegraphics[scale=0.5]{BeamSearchPruning.png}
  \caption{An Example of beam search pruning\\ Iteration 1: Blue Iteration 2: Green, Iteration 3: Yellow}
  \label{fig:beam-prune}
\end{figure}

As we do not know the optimal starting node, we must execute our greedy solution from each of the event nodes.
With this we generated the optimal greedy path through our graph. The results of which can be seen in section \ref{sec:graph-results}.

\subsection{Integer Linear Programming Solution}
Integer Linear Programming (ILP) is a mathematical optimisation where the objective function and the constraints are linear.
ILP is common in the field of optimisation and is ideal for our needs. As ILP aims to find the globally most optimal solution, when applied to pathfinding it
should generate the best global path through all nodes, rather than finding the next most optimal node like in a greedy solution.


The A\* search algorithm is is special case of the generalisation of branch and bound\cite{balas1983branch} and is derived from the primal-dual algorithm for ILP\cite{ye2012note}.
We chose this algorithm as it it follows the ILP methodology. The algorithm works in a similar way to best-first search, but takes into account the path cost already travelled.
This is beneficial as it will give us the globally optimum path by finding the most probable total path, rather than the most probable next step that we saw when using a
greedy algorithm.

Similarly to the greedy solution, as we do not know the optimal start and end node, we had to run the algorithm with every possible start and end node combination.


\section{Summary}

\chapter{Conclusion}
\section{Results}
From our experiments, we generated results from both our classification and path finding methods.
Given a spread of results from our classification of tuples of events, we used both of our path finding
techniques in each of the resulting graphs. Our results include accuracy's and correlations from
experiments using both the sentences extracted from Wikipedia and just the event titles alone.   

\subsection{Classification}
We measure the results on Table \ref{table:classification-results} are measured by the proportion of true results
(both positive and negative) among the total number of results.

The baseline used for classification is a random assignment. As we use binary classifiers and each class has an even
weight, our random assignment puts exactly half in each of the classes.
With an even estination distribution across the two classes, this makes our baseline accuracy 50\%.%make better 

From our results, we can see that classification using only the event titles have a small spread of results
(mean =47 , standard deviation=15.09) and are centered around the baseline figure.


Our experiments that used the retrieved sentences out perform the results using only titles by large margins.
These experiments are centered around 66 and have (mean = 65.\.{3}, standard deviation = 12.1)

\begin{table}[H]
\centering
\label{table:classification-results}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
  \hline
  Accuracy  & DT (tuple) & DT (triple) & SVM & LR & Perceptron & MLP & Baseline\\
  \hline
  With Articles    & 53\%  & 48\%    & 66\% &  76\% & 66\% & 83\% & 50\%\\
\hline
With Titles & 43\%  & 36\% & 51\%    & 52\% & 46\% & 54\% & 50\%\\
\hline
\multicolumn{3}{@{}p{1.5in}}{\footnotesize DT $=$ Decision Tree}\\
\multicolumn{3}{@{}p{1.5in}}{\footnotesize LR $=$ Logistic Regression}\\

\end{tabular}
\caption{Classification Results}
\end{table}


%\begin{table}[H]
%\centering
%\label{table:classification-results}
%\begin{tabular}{|c|c|c|c|c|c|c|c|}
%  \hline
%  Precision  & DT (tuple) & DT (triple) & SVM & LR & Perceptron & MLP & Baseline\\
%  \hline
%  With Articles    & 0.86 & nan  & 0.8  & 0.82 &  0.82 & 0.82 & 0.5\\
%\hline
%With Titles & 0.318  & \% & 0.42 & 0.12 & 0.18 & 0.78 & 0.5\\
%\hline
%\multicolumn{3}{@{}p{1.5in}}{\footnotesize DT $=$ Decision Tree}\\
%\multicolumn{3}{@{}p{1.5in}}{\footnotesize LR $=$ Logistic Regression}\\
%
%\end{tabular}
%\caption{Precision for Classification Results}
%\end{table}


\subsection{Path Finding}
We measure our path finding results using Kendall's Tau.

Our baseline for put generated paths is 0 when using Kendall's Tau.
This is due to when randomly ordering a set of data it will, on average, score a zero against the true ordering
when using Kendall's Tau.


From the use of ILP pathfinding method, we gathered the results in Table \ref{tabel:ILP-Results}.
As we can see, the use of articles greatly increases the correlation of estimated path with the true path.
These results have a much small spread (mean = 0.50466 , standard deviation = 0.1756) than the ILP pathing of titles alone
(mean = -0.1, standard deviation = 0.219).

\begin{table}[H]
\centering
\label{table:ILP-results}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
  \hline
  A\* Search & DT & SVM &Logistic Regression & Perceptron & MLP & Baseline\\
  \hline
With Articles & 0.695 & 0.419 & 0.3 & 0.376   & 0.7333  & 0\\
\hline
With Titles &0.06  & -0.09 & 0.048 & -0.52  & -0.28 & 0\\
\hline
\end{tabular}
\caption{ILP Pathing Results}
\end{table}

Using the greedy solution to pathfinding, we found our results when using article data to have a much lower spread
than the ILP results (mean = 0.46, standard deviation = 0.05 ). Our results in Table \ref{table:greedy-results} indicate that once
again, using article data aids in the ordering correlations of estimated paths rather than when just using event tiles.
the results from just event tiles shows no significant difference to that of the ILP results
(mean = 0.0348, standard deviation= 0.3357).

\begin{table}[H]
\centering
\label{table:greedy-results}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
  \hline
  Beam Search & DT & SVM &Logistic Regression & Perceptron & MLP & Baseline\\
  \hline
With Articles & 0.42 & 0.5151 & 0.3 &  0.44  & 0.5454  & 0\\
\hline
With Titles & 0.214 & -0.62 & 0.17 & 0.09  & 0.32 & 0\\
\hline
\end{tabular}
\caption{Greedy Pathing Results}
\end{table}

\section{Discussion}
%discuss wikipedia retrieval

As we see in both Tables \ref{table:ILP-results} and \ref{table:greedy-results}, the use of event titles alone
provides no real benefit to classification over a random assignment baseline. This may be due to a number of reasons,
but the key factor on these results is the lack of strong features to learn from. With each event title having an average of
XXX words, the features generated are typically quite sparce. We believe this is the main reason that ordering using
only title information is so poor.

With the use of article data retrieved from Wikipedia we see a drastic increase in accuracy of ordering compaired to using
titles alone. As we see in Tables \ref{table:ILP-results} and \ref{table:greedy-results}, the use of article data
increases the ordering correlation, on average, by XXX. This shows us that the use of contextual data does improve the ability
to succesfully temporally order events.

It is on interest how the two pathing methods used were effected by the type of classifier.
For example, the greedy solution out performed the ILP results, both with and without article data,
when using the graph generated by the perceptron classifier. This is in contract to most other classifiers, exculding
the SVM, in which the ILP results outperform the greedy solution when using articles. This may be due to ...\\

The Decision Tree classifier is also substantially more benefitial to temporal ordering of our data than the classifer
reults would have suggested. Despite this, Mulilayer Perceptrons show a real distinction from the rest in terms of both
classification and temporal ordering. This is due to...


As we originally hypothesised in section \ref{}, the ILP solution to event orderingis typically significantly
better than the Greedy solution. This conforms to our hypothesis as ILp looks for the globally opimal path through
the graph, where as the Greedy solution simply looks for the locally optimum, most likely next node, solution.

The relevency score of the sentences retrieved from Wikipedia seen in Table \ref{table:}, while good does leave a lot of
room from improvment. We believe that given a more relevent sentence set for each event would improve the overall
performance of the system as this irrelivent data may be causing a random offset to classification results.

\section{Further Work}
\subsection{Potential Extension of Data Retrieval}
If this project was to be taken further, we would aim to improve the pool of information we can draw from and mitigate the issues outlined in section \ref{sec:dataIssues}.
To do this we would consider multiple other sources for our data and cross-reference then with one another in order to suppress any misinformation within
a particular article.


\section{Conclusion}
% use the follt owing and \cite{} as above if you use BibTeX
% otherwise generate bibtem entries
\bibliographystyle{plain}
\bibliography{./references.bib}
\end{document}
