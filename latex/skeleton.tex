% to choose your degree
% please un-comment just one of the following
\documentclass[bsc,frontabs,twoside,singlespacing,parskip,deptreport]{infthesis}     % for BSc, BEng etc.
% \documentclass[minf,frontabs,twoside,singlespacing,parskip,deptreport]{infthesis}  % for MInf
\usepackage{cite}
\usepackage{graphicx}
\usepackage{float}
\usepackage{color}
\usepackage{xcolor}
\usepackage{ntheorem}   % for theorem-like environments
\usepackage{mdframed}   % for framing
\usepackage{csquotes}

\usepackage{tcolorbox}% http://ctan.org/pkg/tcolorbox

\begin{document}

\title{Event Ordering in News Articles}

\author{James Robert Friel}
% to choose your course
% please un-comment just one of the following
\course{Artificial Intelligence and Computer Science}
%\course{Artificial Intelligence and Software Engineering}
%\course{Artificial Intelligence and Mathematics}
%\course{Artificial Intelligence and Psychology }   
%\course{Artificial Intelligence with Psychology }   
%\course{Linguistics and Artificial Intelligence}    
%\course{Computer Science}
%\course{Software Engineering}
%\course{Computer Science and Electronics}    
%\course{Electronics and Software Engineering}    
%\course{Computer Science and Management Science}    
%\course{Computer Science and Mathematics}
%\course{Computer Science and Physics}  
%\course{Computer Science and Statistics}    
 
% to choose your report type
% please un-comment just one of the following
\project{Undergraduate Dissertation} % CS&E, E&SE, AI&L
%\project{Undergraduate Thesis} % AI%Psy
%\project{4th Year Disertation}

\date{\today}

\abstract{}
%TODO


\maketitle

\section*{Acknowledgements}
I'd Like to thank Greggs Bakery for being there for me through thick and thin.

\tableofcontents

%\pagenumbering{arabic}


\chapter{Introduction}
\section{The Problem}
Nominal data is not descriptive in nature. This lack of information makes it difficult to assign
a canonical ordering to.
The problem tackled within this dissertation is to test if the use of external data sources can
aid in the ordering of news article headlines.
We will be looking at the usefulness of Wikipedia as an external source in order to timeline
historical events.

Our event data comes from the ``Today in History'' dataset. Each of these data points consists
of an event and a date\\
$[$ ``Alaska becomes 49th State'', ``1959-01-01'' $]$


The problem tackled in this paper is based on the paper \cite{abend2015lexical} and several
techniques used are based from this paper. 

%Nominal data is descriptive in nature, making is difficult to assign an cannonical ordering to.
%The problem tackled in this disertation is the ordering of news article headlines to generate
%a most-probable traversal of a weighted directed graph of these events.
%This problem is based off of the paper \cite{abend2015lexical} and some
%techniques discussed and used henseforth are based off of this paper.


%Our data comes from the Wikipedia ``Today in History'' dataset and the nominal data is retrived
%from wikipedia articles.
%The problem explored within this thesis is that of graphing nature.
%Building a most probable path through each node in an edge-weighted directed graph
%, each node beinga single sentence describing an event from history.
%Examples of these events include:
%\say{Julian claendar begins at Greenwich mean noon}
%and \say{AT\&T broken up into 8 countries}

\section{Aims \& Objectives}

The aim of this project is to experiment with machine reading on external data sources to
discover if it is feasable to extract context from these sources to aid in the linear ordering
of historical events.

We aim to do this be experimenting with machine reading techniques to try and extract the most
relevent contextual information from our data sources.

With this information we intend to experiment with classification methods to optimise estimations
of local orderings before generating a graph from the returned data.

Using the graph generated from our classifier, we will attempt to find the optimim maximum spanning path
through every node. This path will be our estimated ordering of history.

We can use some evaluation technique to see how accurate our results are, compair those to not using
external data sources and to a human rate.


%The core aims of the project were: 
%\begin{itemize}
%  \item Investivage the usefulness of wikipedia text in feature generation
%  \item Generate probable dates For each event
%  \item Order these events linearly
%\end{itemize}

%The further goals of the project were to:
%\begin{itemize}
%  \item Generate edge-weighted directional graphs of the data
%  \item Construct probabilities of maximum spanning walks through the graph
%\end{itemize}

%Some initial stretch goals of the project were to construct an interactive
%interface to the resulting graphs to act as a visual aid for the results.

\section{Testing \& Evaluation}
With our dataset being so large, 6250 entries, it can
easily be split into training, development and testing with no
need for overlap. For this the data is split 10\% training,
10\% development and 80\% for testing.

Evaulation of the system was completed using the
Kendall rank correlation coefficient as is a statistic
used to measure the ordinal association between two
measured quantities. This was easy be applied to our results
by compairing the systems estimated orderings of events
with the label associated with the event from the data.

\section{Contributions}

\chapter{Background}
%NLU as a subjext area
The work undertaken in this project settles nicely within the research space of Natural Language Understanding (NLU).
NLU deals with machine reading comprehension, the problem of dissecting natural language inputs into usable features,
and is considered an AI-hard problem.

Reading comprehension is the ability to read, process and understand text.  
There are two levels of reading comprehension, shallow (low-level) and deep (high-level).
Shallow comprehension involves structural and phonemic recognition along with the processing of sentence and  word
structure.
Deep comprehension involves semantic processing, which happens when we encode the meaning of a word and relate it
to similar words ( See word embedding).
This levels system of comprehension was first proposed by  Fergus I. M. Craik and Robert S. Lockhart \cite{wagner2009beyond}.

One of the earliest known attempts at NLU by a computer \cite{russell1995modern}. Since then, NLU has become an
umbrella term for a diverse range of problems and applications. These applications range from simple tasks, such as
issuing simple commands to robots, to highly complex tasks such as full comprehension of poetry passages.
Most problems within NLU fall somewhere between these extremes and will implement one of the types of comprehension
depending on the task.

Reading comprehension within these tasks is import as these application need not only understand the literal content
being passed to them, but also the contextual meaning within to better understand the nuances of the content.
The idea being that machine reading will be able to provide contextual information to allow better task performance when
involving unstructured data.

Unstructured data is data that has no pre-defined data model or manner. This type of data is
commonly seen in NLU with text mining and natural language rarely forming to any cohesive model \cite{feldman2007text}.
From the data sources typically used with NLU (news articles or free-form speech and text) the unstructured format of
the data is beneficial as structured data typically has little to no subtler meaning \cite{}.
%Bit more about reading comprehension is CS

In recent years there has been a commercial interest in NLU, mostly focusing on news-gathering, text categorising and
content-analysis. These systems vary vastly in terms of difficulty and use of comprehension. 

\chapter{Related Work \& Motivation}
%Papers I have read and why we are doing this
\section{Motivation}
\section{Related Work}


\chapter{Wikipedia As a Data Source}%%Potential Chapter name change
%Intro to the data set
Wikipedia was chosen as the external data source for this project as
due to the nature of the events in our dataset, historical points of note,
a source with a wide and deep range of articles was required.
Ad wikipedia as over 5 million articles \cite{} it is reasonable to assume
that most events will exist within its pages.

Wikipedia is also written in a manner that makes comprehension of events reletivly simple.
It is the key sentences within articles that we will use to estimate a timeline.

\section{Text Retrieval}
Using Wikipedia's article retrieval API, gathering the data from
the site is trivial.
Choosing what information is contextually relevent to the subject is, however, not so straightforward.
Along with looking at how to extract the key entities and actions from our 
headlines, we must also experiment with methods of extracting contextually relevent information from
retrieved articles.

\subsubsection{OpenIE}
Stanfords natural language processing (NLP) suite,
a natural language analysis toolkit,
was selected to aid in extraction of features from the dataset.
Using this toolkit, the Open Information Extraction (OpenIE) from
The University of Washington is used to extract Subjects, Objects
and their relations from our event titles.

OpenIE was chosen due to it having several benifitial porperties.

OpenIE impliments a number of extraction techniques, such as ``noun-mediated extraction'' and ``BLAH BALH''.
We have seen through experimentation with the various
extraction methods that there is no one method that gives reasonable results for all of our inputs.
For example using SOME TECHNIQUE of SENTENCE IT DID A BAD THING BUT DID A GOOD THING FOR ANOTHER EXAMPLE.
The ability to use any and all of the avialble techniques seamlessly is benefitial as it allows us to
extract the best possible relationships from each event. 

OpenIE does not require a schema to be specified in advance, this is useful for our purposes
as we do not know the content that will be looked at. This will allow the system to extract
more complex relationships, such as those seen in Figure \ref{fig:compount-relations}, if such
a relationship can be found.

OpenIE can also deal with much more than simple relations,
it can look at positive or negative assertion within these relations which may be benefitial to our
solution.

It was these properties that make OpenIE the optimal system to extract the key relations from our events.
%From our events, we typically saw only simple, linear relations. This made finding the relevent articles
%simple as often only one article was found for each Object and subject in the relationship. More complex
%relations, often with sub relations, can retrieve more article but, depending on the content, still retrieve
%only two articles.


\begin{figure}[H]
  \centering
  \ref{fig:compound-relations}
  \includegraphics[scale=0.4]{openie-train.png}
  \caption{Example of a compound relationship \protect{\cite{OpenIE}}}%\cite{OpenIE}
  \end{figure}

\subsubsection{Article Retrieval}
Using the Objects and Subjects generated by OpenIE we needed to get the relevent Wikipedia Articles.
To do this we used fuzzy searching of Wikipedia, through the use of their API, to get the articles relivent
for these Objects and Subjects.

Several different methods of extracting key sentences were experimented with.
These included:
\begin{enumerate}
\item Extracting only sentences that had a date within them
\item Extracting only sentences that had the other party in the relation within them
\item Extracting only sentences that had a date and the other party within them
\item Extracting paragaphs that referenced the other party
  \item extracting sentences that contained the object or the subject 
  \item extracting sentences that contained the object or the subject or referenced the action between the two 
  \end{enumerate}

%Through the experimentation, we found mixed results in terms of number of sentences retrieved and the relevency
%there in.

As we can see from Table \ref{table:retrieval}, experimentation yeilded mixed results.
With such a large range of average number of sentences retrieved and the relevency there in.
Relevency was measured manually using Cohen's Kappa \cite{}.

Cohen's kappa measures the agreement between two raters who each classify N items into C mutually exclusive categories
\begin{equation}
  \kappa = \frac{p_{0} - p_{e}}{1 - p_{e}}
  \end{equation}

\begin{table}[h]
\centering
\caption{Retrieval Methods and their results}
\label{table:retrieval}
\begin{tabular}{|l|l|l|}
  \hline
Method & Average \# Sentences Retrieved & Relevency \\
\hline
1      &                                &           \\
2      &                                &           \\
3      &                                &           \\
4      &                                &           \\
5      &                                &           \\
6      &                                &           \\        
\hline
\end{tabular}
\end{table}

From these results, method 6 was chosen as our context extraction method. While the
results from this method are not perfectly relevent and other improvments could be made,
time restirctions were a deciding factor in not persuing this experimentation further.

\section{Feature Representation}\label{sec:representation}
Having extracted a series of sentences for each event in our data, we required a concise method
of representing these sentences. It is standard in NLP and ML domains to represent features as vectors.
Several different techniques were discussed, but ultimately due to time constraints only two methods were
experimented with: Bag-Of-Words and Word Embedding.
\subsection{Bag-Of-Words}
The Bag-of-Words model is a simplifying representation of text.
In this model, we take a sentence, or multiple sentences, and represent them as a bag(multiset) of its words.
A multiset is similar to set, but can allow multiple of the same item within.
For example, \{$\alpha,\beta$\} and \{$\alpha,\beta,\alpha$\} are the same set, but individual multisets.
This technique disregards grammar and order, but keeps multiplicity. It is a common technique in Computer vision
and other learning situations \cite{}.


%Need to talk a bit more abput how we are making tuples of every event set
For our project, we implemented Bag-of-Words by using the relevant sentences for both of our events as a feature.
We chose to represent our multisets are counts over the vocabulary so as to build numerical features to easily
use the data with classifiers.
\newpage
\begin{tcolorbox}[width=\textwidth,
                  %%enhanced,
                  %%frame hidden,
                  interior hidden,
                  boxsep=10pt,
                  left=0pt,
                  right=0pt,
                  top=2pt,
                  ]%%
  Given the sentences \vspace{0.5em}\\
  \begin{center}``John likes to watch movies. Mary likes movies too.''\\
  ``John also likes to watch football games.''\vspace{1em}\\
   \end{center}
    Based on these two text documents, a vocabulary list is constructed as follows:\vspace{1em}\\
    \begin{center}
  [``John'',``likes'',``to'',``watch'',``movies'',``also'',``football'',``games'',``Mary'',``too'']\vspace{1em}\\
\end{center}
    From this list a vector is constructed of the occurances
    of each word across the sentences\vspace{1em}\\
 \begin{center} 
  Result: [2,3,2,2,2,1,1,1,1,1]
\end{center}
\end{tcolorbox}

As Bag-Of-Words only looks at single words at time and the contextual data may be lost through this method, we looked at
alternative methods to improve on this.

\subsubsection{N-Grams}
When experimenting with N-Grams, we aimed to try and capture some of the meaning of phrases that appeared in the gathered
sentences.
This was achieved by looking for phrases and sequences of words that recurred throughout the sentences - under the assumption
that what reoccurs must be important. Due to the wide variety of data we were seeing, simple 2 or 3 N N-Grams provided
little to no use, returning junk phrases if anything at all.

With the failure of N-Grams at this problem, we attempted to implement Skip-Grams from the paper \cite{guthrie2006closer}.
By missing out 1 or 2 words in the middle of key words we hoped that we could extract more important word relations to
incorporate into our features.
%Something about why this wasn't better.

This representation worked well as it it provides a numerical representation of the cross-section of our two articles
but does remove some of the contextual data we might receive.

\subsection{Word Embedding}
Word Embedding was also considered ...


\section{Potential Issues With Data Retrieved}\label{sec:dataIssues}
Wikipedia was a good choice for our external data source due its large catalogue of events.
This provided us with more than sufficient data to gather from.
While we have seen all of these positive atributes of wikipedia, there are certain issues that may have to be addressed
that, if the content domain was to be altered, may cause issue.

As Wikipedia is community managed, anybody can edit any page. This has been know to cause 'edit wars' over users
uploading biast information about subjects \cite{}.
While in our current domain of long-term history, the facts and timelines are fairly solid, if we were to apply this project
to a much more current or hotly contested subject - such as the OJ simpson trial, or recent presidential election - Wikipedia
may not be the most appropriate source of information due to these bias edit wars.
%Example


\section{Potential Extention of Data Retrieval}
If this project was to be taken further, we would aim to improve the pool of information we can draw from and mitigate the issues outlined in section \ref{sec:dataIssues}.
To do this we would consider multiple other sources for our data and cross-reference then with one another in order to suppress any misinformation within
a particular article.


\chapter{Learning}
Waffel waffel intro
\section{Approach}
In order to be able to train a classifier on our data, a method of giving each feature a label must be established.

 Given that each event in our data set is of the form
  \begin{eqnarray}
  (t_{i},d_{i}) \hspace{0.5em}for\hspace{0.5em} i \hspace{0.2em}\epsilon \hspace{0.2em} [M]\nonumber
    \end{eqnarray}
    where t is the title, d is the associated date and M is the original data set,

    Using the Wikipedia extraction techniques discussed in section \ref{sec:retrieval} we constructed a new data set
    \begin{eqnarray}
      \{(t_{i},s_{i},d_{j})\} \hspace{0.5em} for \hspace{0.5em}i,j  \hspace{0.2em}\epsilon \hspace{0.2em}[M]\nonumber
    \end{eqnarray}
    
    This formed the basis of our data to generate features.

    From this we built a new data set
    \begin{eqnarray}
      \{(t_{i},s_{i},t_{j},s_{j},b_{ij})\} \hspace{0.5em} for \hspace{0.5em} i,j \hspace{0.2em}\epsilon \hspace{0.2em}[M]\nonumber
      \end{eqnarray}
    where $b_{ij} = [y_{i} > y_{j}]$ indicating which event came first.
    We used this value as the label for our training data and to evaluate our results against.
    A similar data set was constructed using only the article headlines.
    
    From each on the entries in this new data set, we use the feature representation techniques discussed in section \ref{sec:representation} to build numerical
    features to train our classifier with.

Along with building a tuple event dataset, we experimented with triple event entries.
\begin{eqnarray}
      \{(t_{i},s_{i},t_{j},s_{j},t_{k},s_{k}b_{ijk})\} \hspace{0.5em} for \hspace{0.5em} i,j,k \hspace{0.2em}\epsilon \hspace{0.2em}[M]\nonumber
\end{eqnarray}

Where $b_{ijk} = [y_i > y_j > y_k]$ indicating if the ordering is correct for these events.


As we expected, increasing the number of events associated with each training point greatly decreased the accuracy of results.
%Some stuff (paper?) about why adding more events makes things worse?
Due to this, we will not focus on discussing these results, but instead focus on the results gathered from the tuple dataset.

We split our dataset into 80\% Training, 10\% Dev, 10\% Test. This beaks our 6226 entry dataset into 4982,622,622 for the respective split.
The alotment of the entries into each category was done randomly.

\section{Local Learning}
We initially looked at local learning, as opposed to global learning, to try and classify our data.
Local learning is ...
We chose to look at local learning techniques as .... 
\subsection{Decision Trees}
Decision trees were looked at as they are a non-parametric, supervised learning method
that are commonly used for classification and regression. They learn simple decision rules inferred from
data features to create a model that predicts the value of a target variable.

These attributes were promising for our experiments as they fit with our data and requirements, generating
classifications of test data and probabilities of these estimations.

%Move this section Elsewhere
It was also decided to run the experiment using three events, rather that two to see how these
effected the results. 

Some issues found with decision trees were that the probabilities generated were too narrowly distributed
to infer any confidence in the paths generated. Due to this, it was decided to explore other methods of
classification.
%\subsection{Random Forests}

\subsection{Support Vector Machines}
Similarly to Decision trees, Support Vector Machines (SVMs) provide supervised learning models based on
associated learning algorithms for classification and regression.

SVMs were experimented with as given data with a binary classification build the points in space,
making a non-probabilistic binary linear classifier.

This proved beneficial as the results generated were better than the decision trees \ref{table:local-learning}.
SVMs also provided much better classification probability, which is favourable for path generation.
%\subsection{Linear Regression}
% To Be Completed in due course
\vspace{4em}
\subsection{Regression}
Logistic regression was also experimented with as it utilises a binary dependent variable - that being where it
can only take one two values, ``1'' or ``0'' in our case to represent correct ordering.
The logistic regression model estimates the probability of a binary responce based on one or more independent features.
This is benefitial as along with providing clasification estimates, the model can produce probabilities for its
decision. These probabilities can allow us to use weighting techniques to improve our path building.

\subsection{Results}\label{sec:local-results}

Having experimented with the various classification methods outlined above, we can see from table \ref{table:local-learning}
That the use of the retrieved sentences is significantly better at predicting the ordering of tuples of events than that of
just titles alone.

When using the event titles alone, our results drop below our baseline on 50\% accuracy.
Our baseline is the result if every event tuples was randomly assigned an ordering (1 or 0).
These poor results for titles alone indicate that using such small amounts of data
(mean=8.242531320269837,standard deviation= 3.2204518727095195) makes it difficult to make any reasonable inferences to specify the ordering.


As we discussed earler, the difference between using features of two and three events is notable (5\%).

The use of the retrieved article data has a noticable positive effect on the accuuracy of our classifiers.



As we can see from our results, classification of tuple event orderings improves from the baseline by a significant margin.
These results are promising as, the better the individual pauir orderings are, the better the agreement of a traveral across all events will be.
With such a large spread of accuracies for our experiemnts with the retrieved articles, it was decided to explore global learning techniques

%\subsubsection{Kendall Rank Correlation Coefficient}
%For evaluation of our classifier, we decided to use the Kendall rank correlation coefficient (Tau coefficient).
%This statistic was chosen as it can be used to measure the ordinal association between two measured quantities.


%The Tau coefficient is defined as
%\begin{eqnarray}\tau = \frac{(number of concordant pairs) - (number of discordant pairs)}{n(n-1)/2}$\cite{kendalltau}\end{eqnarray}%https://www.encyclopediaofmath.org/index.php/Kendall_tau_metric

%The Tau coefficient was used as it measures results in terms of agreement in the range $-1 \leq \tau \geq 1$.
%This allows us to show the agreement of our orderings, with 1 being a perfect agreement of the ordering, -1 being a perfect disagrement of the ordering and 0 claiming
%total independence.

\begin{table}[H]
\centering
\label{table:local-learning}
\begin{tabular}{|p{5em}|l|l|l|p{4em}|l|}
  \hline
  Accuracy  & DT* (tuple) & DT* (triple) & SVM & Logistic Regression & Baseline\\
  \hline
  With Articles    & 53\%  & 48\%    & 66\% &  76\% & 50\%\\
\hline
With Titles & 43\%  & 36& 51\%    & 52\% & 50\%\\
\hline
\end{tabular}
\caption{Local Learning Results  \\*DT = Decision Tree }
\end{table}


\section{Global Learning}
Global learning is a machine learning method that fits a distribution over the data.
some waffel \cite{huang2008machine}

\subsection{Structured Perceptron}
The Perceptron is a common algorithm used for large-scale learning.
It is a type of linear classifier that bases predictions on a linear predictor function that combines a set of optimised weights
and the input feature vector.
It is primarily used for supervised learning of binary classification.
This is something that will fit well with our data as the perceptron decides whether an input vector
corresponds to a specific class \cite{freund1999large}.

The percepron model has several attributes that will aid in improving the accuracy of our results.
These include:
\begin{itemize}
\item It does not require a learning rate.
\item It is not regularized (penalized).
\item It updates its model only on mistakes
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{PerceptronExample.png}
  \caption{A diagram showing a perceptron updating its linear boundary as more training examples are added. \protect{\cite{goodspeed_2015}}}
\end{figure}


It is these attributes that made the Perceptron a suitable model to experiment with.
For our experimentation, we trained the Perceptron model using  10 iterations with no regularisation term.
\subsection{Neural Networks}
Having discussed the use of Perceptrons, the next logical method to experiment with was\\ 
TODO - talk about the parameters used


\subsection{Results}
As we can see from table \ref{table:global-learning},  the use of global learning techniques greatly improves the accuracy
of classification to the results seen in section \ref{sec:local-results} and the baseline.

\begin{table}[H]
\centering
\label{table:global-learning}
\begin{tabular}{|p{5em}|l|l|p{4em}|l|}
  \hline
  {\small Accuracy}  & {\small Perceptron} & {\small MLP} & Baseline\\
  \hline
{\small With Articles}    & 66\%  & 83\% & 50\%\\
\hline
{\small With Titles} & 46\%  & 54\% & 50\%\\
\hline
\end{tabular}
\caption{Global Learning Results}
\end{table}

From the results of our experiments with various different classification techniques, we have seen that the use of the sentences retrieved
from Wikipedia aid in the classification of pairs of events. It is with this information in mind that we decided to take the results from
the MLP classifications and use those from now to complete the pathing of our timeline.

\chapter{Graphing}
From our classifier, we have constructed a directional ordering between every two events.
By placing each of our events in a node and placing each individual ordering on an edge, we constructed
a digraph of the data.
A digraph is an ordered pair G = (A,V) where\cite{bang2008digraphs}
\begin{itemize}
  \item V is the set of nodes
  \item A is a set of ordered pairs of vertices
\end{itemize}
An example of such a graph can be seen in figure \ref{fig:digraph}.

It is with this new graph that we began to experiment with methods to construct the most probably path through every
node in the graph.


\begin{figure}
  \centering
  \includegraphics[]{}
  \caption{A small example of our results places in a digraph}
  \label{fig:digraph}
  \end{figure}

\section{Traveling Salesman Problem}
The Travelling Salesman Problem (TSP) is a classic NP-hard algorithm problem\cite{junger1995traveling}.%something about NP hard problems
It is defined as\\
\begin{center}
\enquote{A traveling salesman wants to visit each of a set of towns exactly once, starting
from and returning to his home town. One of his problems is to find the shortest
such trip. - \cite{junger1995traveling}}
\end{center}

While the TSP algorithm is still unsolved, we fortunetly only had to solve a a modidified version of the problem.\\

\begin{center}
  \enquote{Given a graph with directed edges, find the optimal path from some starting node through all other nodes in
  the graph.}
\end{center}

These restriction on the graph make finding an optimal path through the graph much simpler.

As we have no fixed start or end point to our path, several common pathing algorithms had to eliminated.
From this it was decided that we would reconstruct the graph inverted edge-weight ($\frac{1}{W}$ where W is the edge weight
between two nodes) and discuss solving the problem using shortest path algorithms.


Given that we now have a directed, weighted and cyclic graph and are trying to find the minimum spanning path, it was
obvious to use Dijkstra's algorithm to find the optimal path.

Dijkstras algorithms is a well-known algorithm for finding the shortest path between two nodes in a graph.
The algorithm requires a graph and a start node and will find the shortest path to all other nodes in the graph.
The runtime for this algorithm is $\mathcal{O}(|E| + |V|log(|V|))$ where E is the number of edges in the graph and V
is the number of veritces.
In our graph these values are $\frac{n(n-1)}{2}$ for E and n for V.

Given that Dijkstra's algorithm requires a fixed start node we ran the algorithm between each node to find
the optimal path. While this in not the most efficient method of finding the optimal path through the graph, adding $n$ to
the runtime.

\section{Encoding}
\subsection{Greedy}
\subsection{ILP}
\section{Results}

\chapter{Results}
\section{Results}
\section{Discussion}
\chapter{Conclusion}
\section{Further Work}
\section{Conclusion}
% use the following and \cite{} as above if you use BibTeX
% otherwise generate bibtem entries
\bibliographystyle{apalike}
\bibliography{./references.bib}
\end{document}
